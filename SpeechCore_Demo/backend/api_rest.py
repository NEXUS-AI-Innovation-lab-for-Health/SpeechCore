#!/usr/bin/env python3
"""
API REST FastAPI pour transcription audio
+ Extraction avec Ollama
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict
from pathlib import Path
import tempfile
import shutil
import httpx
import json
import re

from transcription_engines import transcrire_vosk, transcrire_whisper, transcrire_gladia
from utils import generer_json

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODÃˆLES PYDANTIC POUR EXTRACTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FormField(BaseModel):
    """DÃ©finition d'un champ de formulaire"""
    name: str
    label: str
    type: str
    required: bool = False
    semantic_hint: Optional[str] = None

class FormSchema(BaseModel):
    """Structure complÃ¨te d'un formulaire"""
    fields: List[FormField]

class ExtractionRequest(BaseModel):
    """RequÃªte pour extraire des donnÃ©es d'un texte"""
    form: FormSchema
    text: str

class ExtractionResponse(BaseModel):
    """RÃ©sultat de l'extraction"""
    data: Dict[str, Optional[str]]
    success: bool = True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLIENT OLLAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMClient:
    """Client pour communiquer avec Ollama (serveur local d'IA)"""
    def __init__(self, model="mistral"):
        self.model = model
        self.url = "http://localhost:11434/api/generate"

    async def generate(self, prompt: str) -> str:
        """Envoie un prompt Ã  Ollama et rÃ©cupÃ¨re la rÃ©ponse"""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }

        try:
            async with httpx.AsyncClient(timeout=120) as client:
                response = await client.post(self.url, json=payload)
                response.raise_for_status()
                return response.json()["response"].strip()
        except Exception as e:
            raise HTTPException(
                status_code=500, 
                detail=f"Erreur Ollama : {str(e)}. VÃ©rifiez que 'ollama serve' est lancÃ©."
            )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACTEUR DE FORMULAIRE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FormExtractor:
    """Utilise l'IA (Mistral via Ollama) pour extraire des informations"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def build_prompt(self, form: FormSchema, text: str) -> str:
        """Construit le prompt pour l'IA"""
        fields_description = []
        for field in form.fields:
            hint = field.semantic_hint or field.label
            fields_description.append(f'- "{field.name}": {hint}')
        
        fields_str = '\n'.join(fields_description)
        
        prompt = f"""Tu es un assistant mÃ©dical spÃ©cialisÃ© dans l'extraction d'informations.

Voici un texte d'une consultation mÃ©dicale :
"{text}"

Extrait les informations suivantes et RÃ‰PONDS UNIQUEMENT avec un objet JSON valide :
{fields_str}

Si une information n'est pas prÃ©sente dans le texte, utilise null.

Format de rÃ©ponse (JSON uniquement, sans autre texte) :
{{
  "field_name": "valeur extraite ou null"
}}

Exemple :
Texte : "Le patient s'appelle Jean Dupont, il a 45 ans"
RÃ©ponse : {{"nom": "Dupont", "prenom": "Jean", "age": "45"}}

IMPORTANT : RÃ©ponds UNIQUEMENT avec le JSON, sans explication."""

        return prompt

    def parse_llm_output(self, raw_output: str) -> dict:
        """Parse la rÃ©ponse de l'IA pour extraire le JSON"""
        if not raw_output:
            return {}

        # Chercher le JSON dans la rÃ©ponse
        match = re.search(r"\{.*\}", raw_output, re.DOTALL)
        if not match:
            return {}

        json_str = match.group(0)

        try:
            data = json.loads(json_str)
        except json.JSONDecodeError:
            return {}

        # Normaliser : tout en string ou None
        normalized = {}
        for key, value in data.items():
            if value is None or str(value).lower() in ["null", "none", "n/a"]:
                normalized[key] = None
            else:
                normalized[key] = str(value)

        return normalized

    async def extract(self, form: FormSchema, text: str) -> dict:
        """Fonction principale d'extraction"""
        prompt = self.build_prompt(form, text)
        
        print(f"ğŸ“¤ Envoi Ã  Ollama...")
        
        raw_output = await self.llm.generate(prompt)
        
        print(f"ğŸ“¥ RÃ©ponse brute : {raw_output[:200]}...")
        
        data = self.parse_llm_output(raw_output)
        
        # Garantir que TOUS les champs existent
        result = {}
        for field in form.fields:
            result[field.name] = data.get(field.name)

        return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INITIALISATION FASTAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="API de Transcription Audio + Extraction",
    description="Vosk, Whisper, Gladia + Ollama",
    version="3.0.0"
)

# CORS : permettre les requÃªtes depuis React
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, spÃ©cifier les domaines autorisÃ©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialiser Ollama pour l'extraction
llm_client = LLMClient()
extractor = FormExtractor(llm_client)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ROUTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    return {
        "message": "API de Transcription Audio v3",
        "endpoints": {
            "/vosk": "Transcription Vosk (local, rapide)",
            "/whisper": "Transcription Whisper (local, GPU)",
            "/gladia": "Transcription Gladia (cloud, 10h/mois)",
            "/extract": "Extraction de donnÃ©es avec Ollama"
        },
        "documentation": "/docs"
    }


@app.get("/health")
async def health():
    """Route de santÃ©"""
    return {
        "status": "ok",
        "message": "Serveur opÃ©rationnel",
        "endpoints": ["vosk", "whisper", "gladia", "extract"]
    }


@app.post("/vosk")
async def transcription_vosk(
    file: UploadFile = File(...),
    modele: str = "grand",
    nb_locuteurs: int = 2,
    reduction_bruit: bool = True,
    type_environnement: str = "2",
    methode_bruit: str = "noisereduce"
):
    """
    Transcription avec Vosk
    
    - **modele**: petit ou grand
    - **nb_locuteurs**: 1-10
    - **reduction_bruit**: true/false
    - **type_environnement**: 1=Silencieux, 2=Normal, 3=Bruyant, 4=Constant
    - **methode_bruit**: noisereduce ou silero
    """
    if not file.filename.endswith('.wav'):
        raise HTTPException(status_code=400, detail="Fichier WAV requis")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
        shutil.copyfileobj(file.file, tmp)
        tmp_path = Path(tmp.name)
    
    try:
        resultats = transcrire_vosk(
            tmp_path,
            modele=modele,
            nb_locuteurs=nb_locuteurs,
            reduction_bruit=reduction_bruit,
            type_environnement=type_environnement,
            methode_bruit=methode_bruit
        )
        
        json_response = generer_json(file.filename, resultats, "Vosk")
        return JSONResponse(content=json_response)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


@app.post("/whisper")
async def transcription_whisper(
    file: UploadFile = File(...),
    config: str = "cpu_rapide",
    nb_locuteurs: int = 2,
    reduction_bruit: bool = True,
    type_environnement: str = "2",
    methode_bruit: str = "noisereduce"
):
    """
    Transcription avec Whisper
    
    - **config**: cpu_rapide, cpu_qualite, gpu_equilibre, gpu_max, ultra_rapide
    - **nb_locuteurs**: 1-10
    - **reduction_bruit**: true/false
    - **type_environnement**: 1=Silencieux, 2=Normal, 3=Bruyant, 4=Constant
    - **methode_bruit**: noisereduce ou silero
    """
    if not file.filename.endswith('.wav'):
        raise HTTPException(status_code=400, detail="Fichier WAV requis")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
        shutil.copyfileobj(file.file, tmp)
        tmp_path = Path(tmp.name)
    
    try:
        resultats = transcrire_whisper(
            tmp_path,
            config=config,
            nb_locuteurs=nb_locuteurs,
            reduction_bruit=reduction_bruit,
            type_environnement=type_environnement,
            methode_bruit=methode_bruit
        )
        
        json_response = generer_json(file.filename, resultats, "Whisper")
        return JSONResponse(content=json_response)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


@app.post("/gladia")
async def transcription_gladia(
    file: UploadFile = File(...),
    nb_locuteurs: int = 0
):
    """
    Transcription avec Gladia API
    
    - **nb_locuteurs**: 0 pour auto, 1-10 pour fixe
    """
    if not file.filename.endswith('.wav'):
        raise HTTPException(status_code=400, detail="Fichier WAV requis")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
        shutil.copyfileobj(file.file, tmp)
        tmp_path = Path(tmp.name)
    
    try:
        resultats = transcrire_gladia(tmp_path, nb_locuteurs=nb_locuteurs)
        json_response = generer_json(file.filename, resultats, "Gladia")
        return JSONResponse(content=json_response)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


@app.post("/extract", response_model=ExtractionResponse)
async def extract_form(req: ExtractionRequest):
    """
    Extrait automatiquement des donnÃ©es d'un texte pour remplir un formulaire
    
    Utilise Ollama (Mistral) pour analyser le texte et extraire les valeurs.
    
    Exemple de requÃªte :
    {
      "form": {
        "fields": [
          {"name": "nom", "label": "Nom du patient", "type": "text"},
          {"name": "age", "label": "Ã‚ge", "type": "number"}
        ]
      },
      "text": "Le patient s'appelle Jean Dupont et il a 45 ans"
    }
    
    RÃ©ponse :
    {
      "data": {
        "nom": "Dupont",
        "age": "45"
      },
      "success": true
    }
    """
    try:
        print(f"ğŸ“ Extraction demandÃ©e pour {len(req.form.fields)} champs")
        print(f"ğŸ“„ Texte : {req.text[:100]}...")
        
        # Appeler l'extracteur
        data = await extractor.extract(req.form, req.text)
        
        print(f"âœ… Extraction terminÃ©e : {data}")
        
        return {"data": data, "success": True}
    
    except Exception as e:
        print(f"âŒ Erreur extraction : {e}")
        raise HTTPException(status_code=500, detail=str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LANCEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   API SPEECHCORE COMPLÃˆTE                           â•‘
    â•‘                                                       â•‘
    â•‘   http://localhost:8000                              â•‘
    â•‘                                                       â•‘
    â•‘   ROUTES DISPONIBLES :                                â•‘
    â•‘   â€¢ POST /vosk      - Transcription Vosk             â•‘
    â•‘   â€¢ POST /whisper   - Transcription Whisper          â•‘
    â•‘   â€¢ POST /gladia    - Transcription Gladia           â•‘
    â•‘   â€¢ POST /extract   - Extraction Ollama              â•‘
    â•‘                                                       â•‘
    â•‘   ğŸ“š Documentation : http://localhost:8000/docs      â•‘
    â•‘                                                       â•‘
    â•‘   âš ï¸  PRÃ‰REQUIS :                                     â•‘
    â•‘   â€¢ Ollama : ollama serve                            â•‘
    â•‘   â€¢ ModÃ¨le : ollama pull mistral                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)