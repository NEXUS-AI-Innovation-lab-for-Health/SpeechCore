#!/usr/bin/env python3
"""
Serveur FastAPI unifiÃ© pour SpeechCore
Combine :
- Transcription audio avec Whisper
- Extraction automatique de donnÃ©es avec Ollama/Mistral
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
import tempfile
import os
import whisper
import httpx
import json
import re

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODÃˆLES DE DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FormField(BaseModel):
    """DÃ©finition d'un champ de formulaire"""
    name: str                           # Nom technique (ex: "patient_name")
    label: str                          # Label affichÃ© (ex: "Nom du patient")
    type: str                           # Type de champ (text, number, date, etc.)
    required: bool = False              # Obligatoire ou non
    semantic_hint: Optional[str] = None # Indice sÃ©mantique pour l'IA

class FormSchema(BaseModel):
    """Structure complÃ¨te d'un formulaire"""
    fields: List[FormField]

class ExtractionRequest(BaseModel):
    """RequÃªte pour extraire des donnÃ©es d'un texte"""
    form: FormSchema  # Structure du formulaire
    text: str         # Texte transcrit Ã  analyser

class ExtractionResponse(BaseModel):
    """RÃ©sultat de l'extraction"""
    data: Dict[str, Optional[str]]  # Valeurs extraites pour chaque champ
    success: bool = True

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLIENT OLLAMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMClient:
    """
    Client pour communiquer avec Ollama (serveur local d'IA)
    Ollama doit Ãªtre lancÃ© avec : ollama run mistral
    """
    def __init__(self, model="mistral"):
        self.model = model
        self.url = "http://localhost:11434/api/generate"

    async def generate(self, prompt: str) -> str:
        """
        Envoie un prompt Ã  Ollama et rÃ©cupÃ¨re la rÃ©ponse
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False  # Pas de streaming, on veut tout d'un coup
        }

        try:
            async with httpx.AsyncClient(timeout=120) as client:
                response = await client.post(self.url, json=payload)
                response.raise_for_status()
                return response.json()["response"].strip()
        except Exception as e:
            raise HTTPException(
                status_code=500, 
                detail=f"Erreur Ollama : {str(e)}. VÃ©rifiez que 'ollama run mistral' est lancÃ©."
            )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACTEUR DE FORMULAIRE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class FormExtractor:
    """
    Utilise l'IA (Mistral via Ollama) pour extraire automatiquement
    des informations d'un texte et remplir un formulaire
    """
    
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client

    def build_prompt(self, form: FormSchema, text: str) -> str:
        """
        Construit le prompt qui sera envoyÃ© Ã  l'IA
        
        Le prompt explique Ã  l'IA :
        - Quels champs chercher
        - Le texte Ã  analyser
        - Le format de rÃ©ponse attendu (JSON)
        """
        fields_description = []
        for field in form.fields:
            hint = field.semantic_hint or field.label
            fields_description.append(f'- "{field.name}": {hint}')
        
        fields_str = '\n'.join(fields_description)
        
        prompt = f"""Tu es un assistant mÃ©dical spÃ©cialisÃ© dans l'extraction d'informations.

Voici un texte d'une consultation mÃ©dicale :
"{text}"

Extrait les informations suivantes et RÃ‰PONDS UNIQUEMENT avec un objet JSON valide :
{fields_str}

Si une information n'est pas prÃ©sente dans le texte, utilise null.

Format de rÃ©ponse (JSON uniquement, sans autre texte) :
{{
  "field_name": "valeur extraite ou null"
}}

Exemple :
Texte : "Le patient s'appelle Jean Dupont, il a 45 ans"
RÃ©ponse : {{"nom": "Dupont", "prenom": "Jean", "age": "45"}}

IMPORTANT : RÃ©ponds UNIQUEMENT avec le JSON, sans explication."""

        return prompt

    def parse_llm_output(self, raw_output: str) -> dict:
        """
        Parse la rÃ©ponse de l'IA pour extraire le JSON
        
        Parfois l'IA rajoute du texte avant/aprÃ¨s le JSON,
        donc on cherche le JSON avec une regex
        """
        if not raw_output:
            return {}

        # Chercher le JSON dans la rÃ©ponse
        match = re.search(r"\{.*\}", raw_output, re.DOTALL)
        if not match:
            return {}

        json_str = match.group(0)

        # Parser le JSON
        try:
            data = json.loads(json_str)
        except json.JSONDecodeError:
            return {}

        # Normaliser : tout en string ou None
        normalized = {}
        for key, value in data.items():
            if value is None or str(value).lower() in ["null", "none", "n/a"]:
                normalized[key] = None
            else:
                normalized[key] = str(value)

        return normalized

    async def extract(self, form: FormSchema, text: str) -> dict:
        """
        Fonction principale d'extraction
        
        1. Construit le prompt
        2. Envoie Ã  l'IA
        3. Parse la rÃ©ponse
        4. Garantit que tous les champs sont prÃ©sents
        """
        # Construire le prompt
        prompt = self.build_prompt(form, text)
        
        print(f"ğŸ“¤ Envoi Ã  Ollama...")
        
        # Appeler l'IA
        raw_output = await self.llm.generate(prompt)
        
        print(f"ğŸ“¥ RÃ©ponse brute : {raw_output[:200]}...")
        
        # Parser la rÃ©ponse
        data = self.parse_llm_output(raw_output)
        
        # Garantir que TOUS les champs du formulaire existent dans la rÃ©ponse
        result = {}
        for field in form.fields:
            result[field.name] = data.get(field.name)

        return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INITIALISATION FASTAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="API SpeechCore",
    description="API pour transcription audio (Whisper) et extraction de donnÃ©es (Ollama)",
    version="2.0.0"
)

# CORS : permettre les requÃªtes depuis le navigateur
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Charger Whisper au dÃ©marrage
print("ğŸ”„ Chargement du modÃ¨le Whisper (base)...")
whisper_model = whisper.load_model("base")
print("âœ… Whisper chargÃ© !")

# Initialiser le systÃ¨me d'extraction
llm_client = LLMClient()
extractor = FormExtractor(llm_client)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ROUTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "API SpeechCore - Transcription + Extraction",
        "status": "ok",
        "endpoints": {
            "/health": "VÃ©rifier que le serveur fonctionne",
            "/transcribe": "Transcrire un fichier audio avec Whisper",
            "/extract": "Extraire des donnÃ©es d'un texte avec Ollama"
        }
    }

@app.get("/health")
async def health():
    """Route de santÃ©"""
    return {
        "status": "ok",
        "whisper": "loaded",
        "message": "Serveur opÃ©rationnel"
    }

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    """
    Transcrit un fichier audio avec Whisper
    
    Identique Ã  la version simple
    """
    try:
        # Sauvegarder temporairement
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_path = tmp_file.name
        
        print(f"ğŸ“‚ Fichier reÃ§u : {file.filename}")
        
        # Transcrire
        result = whisper_model.transcribe(tmp_path, language='fr', fp16=False)
        text = result["text"].strip()
        
        print(f"âœ… Transcription : '{text[:50]}...'")
        
        # Nettoyer
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)
        
        return {
            "text": text,
            "language": result.get("language", "fr"),
            "success": True
        }
    
    except Exception as e:
        if 'tmp_path' in locals() and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        raise HTTPException(status_code=500, detail=f"Erreur : {str(e)}")

@app.post("/extract", response_model=ExtractionResponse)
async def extract_form(req: ExtractionRequest):
    """
    Extrait automatiquement des donnÃ©es d'un texte pour remplir un formulaire
    
    Exemple de requÃªte :
    {
      "form": {
        "fields": [
          {"name": "nom", "label": "Nom du patient", "type": "text"},
          {"name": "age", "label": "Ã‚ge", "type": "number"}
        ]
      },
      "text": "Le patient s'appelle Jean Dupont et il a 45 ans"
    }
    
    RÃ©ponse :
    {
      "data": {
        "nom": "Dupont",
        "age": "45"
      },
      "success": true
    }
    """
    try:
        print(f"ğŸ“ Extraction demandÃ©e pour {len(req.form.fields)} champs")
        print(f"ğŸ“„ Texte : {req.text[:100]}...")
        
        # Appeler l'extracteur
        data = await extractor.extract(req.form, req.text)
        
        print(f"âœ… Extraction terminÃ©e : {data}")
        
        return {"data": data, "success": True}
    
    except Exception as e:
        print(f"âŒ Erreur extraction : {e}")
        raise HTTPException(status_code=500, detail=str(e))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LANCEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   SERVEUR SPEECHCORE COMPLET                    â•‘
    â•‘                                                   â•‘
    â•‘   http://localhost:8000                          â•‘
    â•‘                                                   â•‘
    â•‘   Routes :                                        â•‘
    â•‘   - GET  /health                                  â•‘
    â•‘   - POST /transcribe (Whisper)                    â•‘
    â•‘   - POST /extract (Ollama/Mistral)                â•‘
    â•‘                                                   â•‘
    â•‘   âš ï¸  Avant de lancer, installer Ollama :        â•‘
    â•‘   https://ollama.com/download                     â•‘
    â•‘   puis : ollama pull mistral                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)